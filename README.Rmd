---
title: "Utilizing a Capture-Recapture Strategy to Accelerate Infectious Disease Surveillance"
#author: "Lin Ge"
#date: "`r Sys.time()`"
output:
  github_document:
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This page aims to illustrate the Data Generation, Analysis, Simulation Study as well as the Numerical Example for the manuscript **"Utilizing a Capture-Recapture Strategy to Accelerate Infectious Disease Surveillance"**. 

All functions are available in this Github repository and can be loaded as follows.

```{r}
source("FUN_CRC_project_SeSp.R")
```

### 1. Data Generation

Suppose we generate a dataset with total population size $N_{total}=1,000$ and true disease prevalence $p=0.1$. Data were generated in such a way that among those with disease, 50% of individuals exhibited symptoms. In contrast, only 10% of those without disease showed symptoms. The Stream 1 sample was drawn to reflect voluntary-based non-representative surveillance data, selecting 80% of individuals with symptoms for testing as opposed to 10% of those without symptoms. Stream 2 was generated as the anchor stream independently of Stream 1, with the sample size $n_2=50$. We then consider "low" level of misclassification (e.g., $Se_1=Sp_1=0.9$, $Se_1=Sp_1=0.95$). This is the same setting as Table 2 in the paper.

Therefore we set the following parameters:
```{r}
N = 1000

p_case = 0.1
p_case_sym = 0.5
p_not_case_sym = 0.1

N_true = c(rep(1,round(N*p_case)),rep(0,round(N*(1-p_case))))
Npos = sum(N_true)
Nneg = N-Npos

# two-steam surveillance 
p_test1_givSym = .8
p_test1_givAsym = .1
Se1 = 0.9
Sp1 = 0.9
  
n2 = 50
p_stream2 = n2/N
p_test2_givSym = p_stream2
p_test2_givAsym = p_stream2
Se2 = 0.95
Sp2 = 0.95
```
and we can generate the dataset (including testing results in Stream 1 and 2, as well as ``n1-n9``) by the following functions.
```{r}
set.seed(1111111)
# generate the symptom status
N_sym = rep(0,N)
N_sym[which(N_true == 1)] = rbinom(Npos,1,p_case_sym)
N_sym[which(N_true == 0)] = rbinom(N-Npos,1,p_not_case_sym)
p_sym = sum(N_sym)/N

# generate steam 1
simu1 = simu_sym3(N,N_sym,p_sym,N_true,p_test1_givSym,p_test1_givAsym,Se1,Sp1)
test1 = simu1$test
testpos1 = simu1$testpos
    
# generate steam 2
simu2 = simu_sym_RS(N,N_true,p_stream2,Se2,Sp2)
test2 = simu2$test
testpos2 = simu2$testpos

obs_summary = two_by_two_table2(test1,testpos1,test2,testpos2)
n1 = obs_summary$n1
n2 = obs_summary$n2
n3 = obs_summary$n3
n4 = obs_summary$n4
n5 = obs_summary$n5
n6 = obs_summary$n6
n7 = obs_summary$n7
n8 = obs_summary$n8
n9 = obs_summary$n9
```

### 2. Data Analysis
There are two estimators compared in Section 2 of the paper: Random Sampling (RS) based estimator and Capture-Recapture (CRC) estimator. We use function `RS_mis` for RS estimator in eqn. (1)-(3), and `ML_est_SESP_numerical` or `ML_est_SESP_closedform` for CRC estimator in eqn. (5)-(8).

As an example, we use the following programs to calculate RS estimator:
```{r}
RS = RS_mis(N, n_stream2=n1+n2+n3+n4+n7+n8, n_pos_stream2=n1+n4+n7, Se2=Se2, Sp2=Sp2)
N_RS = RS$Nhat
N_RS_sd = RS$SEhat
RS = data.frame(est=N_RS, se=N_RS_sd, pct_025=N_RS-1.96*N_RS_sd, pct_975=N_RS+1.96*N_RS_sd,
           width=1.96*N_RS_sd*2)
print(RS,row.names=FALSE) 
```
and we calculate the closed form CRC estimator as follows:
```{r}
MLE = ML_est_SESP_closedform(n1,n2,n3,n4,n5,n6,n7,n8,n9,Se1_Sp1_par=c(Se1,Sp1),
                             Se2_Sp2_par=c(Se2,Sp2))
N_SESP = MLE$mle 
N_SESP_sd = MLE$mle_se
SESP =data.frame(est=N_SESP, se=N_SESP_sd, pct_025=N_SESP-1.96*N_SESP_sd, 
                 pct_975=N_SESP+1.96*N_SESP_sd,width=1.96*N_SESP_sd*2)
print(SESP,row.names=FALSE) 
```
Except for the Wald-type confidence interval for CRC estimator, we also provide a Bayesian credible interval in Section 2.4. 
```{r}
BC_interval = BC_interval_SESP(n1,n2,n3,n4,n5,n6,n7,n8,n9,Se1,Sp1,Se2,Sp2)
BC_interval2 = RS_BC2(N, n_pos_stream2=n1+n4+n7,n_stream2=n1+n2+n3+n4+n7+n8,Se2,Sp2)
if(BC_interval$BC_width>(N * BC_interval2$BC_width)) {
  lower = N * BC_interval2$BC_lower 
  upper = N * BC_interval2$BC_upper 
  BC_interval = list(BC_lower = lower,BC_upper = upper,BC_width = upper - lower)
}

if (BC_interval$BC_upper >= Npos &&
    BC_interval$BC_lower <= Npos) {
  BC_interval_coverage = 1
} else{
  BC_interval_coverage = 0
}
BC_interval_width = BC_interval$BC_width

SESP_bc =data.frame(est=N_SESP, se=N_SESP_sd, pct_025=BC_interval$BC_lower, 
                    pct_975=BC_interval$BC_upper,width=BC_interval_width)
print(SESP_bc,row.names=FALSE) 
```
Meanwhile, we also proposed a MI-based approach when Se and Sp are estimated from the validation data. More details are available in "Numeric Example" below.


### 3. Simulation Study

To perform simulation study, we use the following loop to evaluate the simulation results and generate Table 2-4.

```{r}
tmp = NULL
for(p_case in seq(0.1,0.5,0.2)){
  for(n2_samples in c(50, 100, 300) ){
     p_stream2 = n2_samples/N
     res = find_p_p2(p_case,p_stream2, N, Se1,Sp1=Se1,Se2=Se1+0.05,Sp2=Se1+0.05)
     tmp = rbind(tmp,t(c(p_case,p_stream2,res$res.true,res$res.RS[1:5],res$res.SESP[1:5],
                            res$res.BC_width, res$res.BC_pct, 
                            res$res.SESP2, res$res.SESP2_sd, Se1)))
   }
}
colnames(tmp) = c('p_case','p_stream2','N_true','RS.mean','RS.sd','RS.avgse','RS.width',
                  'RS.Clpct','SESP.mean','SESP.sd','SESP.avgse','SESP.width','SESP.Clpct',
                  'SESP.BCwidth','SESP.BCpct','SESP2.mean','SESP2.sd', 'Se1')
(result = round(tmp[,],2))
```


### 4. Numeric Example

Now we analyze the numerical example provided in this paper. The data can be found in (`data_numerical_example.R`).
```{r}
library(pander)
set.seed(111111)
source("data_numerical_example.R")
```

```{r, echo=F}
## Generate numerical data set

# target population generation
N = 1000

p_case = 0.1#0.2#
p_case_sym = 0.5
p_not_case_sym = 0.1

N_true = c(rep(1,round(N*p_case)),rep(0,round(N*(1-p_case))))

Npos = sum(N_true)
Nneg = N-Npos

# individual symptom info
N_sym = rep(0,N)
N_sym[which(N_true == 1)] = rbinom(Npos,1,p_case_sym)
N_sym[which(N_true == 0)] = rbinom(N-Npos,1,p_not_case_sym)
p_sym = sum(N_sym)/N

# two-steam surveillance 
p_test1_givSym = .8
p_test1_givAsym = .1
Se1 = 0.63
Sp1 = 0.998

p_stream2 = 0.1
Se2 = 0.94
Sp2 = 1

# 1 set of observation

# simu steam 1
simu1 = simu_sym3(N,N_sym,p_sym,N_true,p_test1_givSym,p_test1_givAsym,Se1,Sp1)
test1 = simu1$test
testpos1 = simu1$testpos

# simu steam 2
simu2 = simu_sym_RS(N,N_true,p_stream2,Se2,Sp2)
test2 = simu2$test
testpos2 = simu2$testpos

# print(c(sum(testpos1),sum(testpos2)))

obs_summary = two_by_two_table2(test1,testpos1,test2,testpos2)

n1 = obs_summary$n1
n2 = obs_summary$n2
n3 = obs_summary$n3
n4 = obs_summary$n4
n5 = obs_summary$n5
n6 = obs_summary$n6
n7 = obs_summary$n7
n8 = obs_summary$n8
n9 = obs_summary$n9
```

The data includes nine cell counts (`n1`-`n9`):
```{r, echo=T}
data_obs = c(n1,n2,n3,n4,n5,n6,n7,n8,n9)
data_obs
```
as well as the 2-by-2 table of testing results from Table 5 of paper: True validation data from Murakami et al. (2023),
```{r, echo=T}
n_SE1SP1_validation = c(n11_t1,n10_t1,n01_t1,n00_t1)
matrix(n_SE1SP1_validation,2,2,byrow = T)

Se1_initial = n11_t1/(n11_t1+n01_t1)
Sp1_initial = n00_t1/(n10_t1+n00_t1)
c(Se1_initial,Sp1_initial)
```
and from Casati et al. (2022),
```{r, echo=T}
n_SE2SP2_validation = c(n11_t2,n10_t2,n01_t2,n00_t2)
matrix(n_SE2SP2_validation,2,2,byrow = T)

Se2_initial = n11_t2/(n11_t2+n01_t2)
Sp2_initial = n00_t2/(n10_t2+n00_t2)
c(Se2_initial,Sp2_initial)
```


```{r, echo=F}
N = sum(data_obs)
SESP_par_initial = c(Se1_initial, Sp1_initial, Se2_initial, Sp2_initial)
```

#### 3.1 Proposed Approach for Reliable Se, Sp Parameters
In Section 4, we first analyze the data assuming that the misclassification parameters of each data streams are known. Then we can directly calculate the case count estimates and corresponding confidence (or credible) intervals using the approaches introduced in Section 2. That is,

```{r}
RS = RS_mis(N=sum(data_obs), n_stream2=n1+n2+n3+n4+n7+n8, 
            n_pos_stream2=n1+n4+n7, Se2=Se2_initial, Sp2=Sp2_initial)
N_RS = RS$Nhat
N_RS_sd = RS$SEhat
RS_m1 = data.frame(est=N_RS, se=N_RS_sd, pct_025=N_RS-1.96*N_RS_sd, 
                   pct_975=N_RS+1.96*N_RS_sd, width=1.96*N_RS_sd*2)
print(RS_m1,row.names=FALSE) 
```
and CRC estimate results:
```{r}
MLE = ML_est_SESP_closedform(n1,n2,n3,n4,n5,n6,n7,n8,n9,Se1_Sp1_par=c(Se1_initial,Sp1_initial),
                             Se2_Sp2_par=c(Se2_initial,Sp2_initial))
N_SESP = MLE$mle 
N_SESP_sd = MLE$mle_se
SESP_m1 =data.frame(est=N_SESP, se=N_SESP_sd, pct_025=N_SESP-1.96*N_SESP_sd, 
                    pct_975=N_SESP+1.96*N_SESP_sd, width=1.96*N_SESP_sd*2)
print(SESP_m1,row.names=FALSE) 
```
as well as CRC estimate with Bayesian credible interval:
```{r}
tmp2 = BC_interval_SESP(n1,n2,n3,n4,n5,n6,n7,n8,n9,Se1_initial,Sp1_initial,
                        Se2_initial,Sp2_initial)
SESP_m1_bc = data.frame(est=N_SESP, se=N_SESP_sd, pct_025=tmp2$BC_lower,
                        pct_975=tmp2$BC_upper,width=tmp2$BC_width)
print(SESP_m1_bc,row.names=FALSE) 
```

#### 3.2 MI-based Approach for Estimable Se, Sp Parameters
With access to validation data for estimating the misclassification parameters as in Table 5, we recommend the approach introduced in Section 2.5 that adapts the multiple imputation (MI) paradigm (Rubin, 1987) to account for uncertainty in these parameters. In this repository, we define a function ``MI_main`` to implement MI to calculate each estimators introduced in the paper. The results are as follows.
```{r}
tmp = unlist(MI_main(data_obs,n_SE1SP1_validation,n_SE2SP2_validation))
RS_m2 = tmp[1:5]
SESP_m2 = tmp[6:10]
SESP_m2_bc = tmp[c(6,7,11:13)]
RS_m2
```

```{r}
SESP_m2
```

```{r}
SESP_m2_bc
```


#### 3.3 Numerical Example Results 
Overall, we can combine all results and compare each methods as follows (same as Table 6 in the paper).
```{r, echo=F}
result_summary = rbind(RS_m1=RS_m1,SESP_m1=SESP_m1,SESP_m1_bc=SESP_m1_bc,
                       RS_m2=RS_m2,SESP_m2=SESP_m2,SESP_m2_bc=SESP_m2_bc)
colnames(result_summary) = c('est','se','2.5%','97.5%','width')
result_summary  = round(result_summary,1)
```

```{r, echo=F}
pander(result_summary, style = "rmarkdown", split.table = Inf)
```

